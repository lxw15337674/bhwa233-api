import OpenAI from 'openai';
import { Injectable, BadRequestException, Logger } from '@nestjs/common';
import { AIRequest } from './type';
import { MessageDto } from './dto/summarize.dto';
import puppeteer from 'puppeteer-core';
import chromium from '@sparticuz/chromium';
import MarkdownIt from 'markdown-it';
import { summaryTemplate } from './templates/summary.template';
import { existsSync } from 'fs';
import { join } from 'path';
import axios from 'axios';

const aiPrompt = process.env.AI_PROMPT ?? '';

type ToolExecutor = (toolName: string, args: unknown) => Promise<string>;

interface ToolingOptions {
    tools: OpenAI.ChatCompletionTool[];
    executeTool: ToolExecutor;
    toolChoice?: OpenAI.ChatCompletionToolChoiceOption;
    maxToolRounds?: number;
}

type TavilySearchArgs = {
    query: string;
    max_results?: number;
    search_depth?: 'basic' | 'advanced';
    include_answer?: boolean;
    include_images?: boolean;
    include_raw_content?: boolean;
};

@Injectable()
export class AiService {
    private readonly logger = new Logger(AiService.name);
    private openai: OpenAI;

    constructor() {
        this.openai = new OpenAI({
            baseURL: process.env.AI_BASE_URL,
            apiKey: process.env.AI_API_KEY,
        });
    }

    // æ£€æµ‹æœ¬åœ° Chrome è·¯å¾„ï¼ˆWindowsï¼‰
    private findLocalChrome(): string | null {
        const possiblePaths = [
            process.env.CHROME_PATH,
            join(process.env.PROGRAMFILES || 'C:\\Program Files', 'Google\\Chrome\\Application\\chrome.exe'),
            join(process.env['PROGRAMFILES(X86)'] || 'C:\\Program Files (x86)', 'Google\\Chrome\\Application\\chrome.exe'),
            join(process.env.LOCALAPPDATA || '', 'Google\\Chrome\\Application\\chrome.exe'),
        ];

        for (const path of possiblePaths) {
            if (path && existsSync(path)) {
                this.logger.log(`Found Chrome at: ${path}`);
                return path;
            }
        }

        return null;
    }

    async generateResponse(
        body: AIRequest
    ): Promise<string> {
        const {
            prompt,
            model = process.env.AI_MODEL ?? 'step-3',
            rolePrompt = aiPrompt,
            searchDescription = 'å½“éœ€è¦è·å–å®æ—¶ä¿¡æ¯ã€æœ€æ–°æ–°é—»ã€å½“å‰äº‹ä»¶æˆ–ç”¨æˆ·è¯¢é—®çš„ä¿¡æ¯å¯èƒ½éœ€è¦æœ€æ–°æ•°æ®æ—¶ä½¿ç”¨ç½‘ç»œæœç´¢',
            enableWebSearch = false // æ–°å¢å¼€å…³ï¼Œé»˜è®¤ä¸º false
        } = body;

        // éªŒè¯ prompt æ˜¯å¦ä¸ºç©º
        if (!prompt || prompt.trim() === '') {
            this.logger.error('[AI Service] Empty prompt provided:', { prompt, type: typeof prompt });
            throw new BadRequestException('Prompt cannot be empty');
        }

        if (enableWebSearch) {
            const tools = [
                {
                    type: 'builtin_function',
                    function: {
                        name: '$web_search',
                        description: searchDescription,
                    },
                },
            ] as unknown as OpenAI.ChatCompletionTool[];

            return this.generateResponseWithTools(
                {
                    prompt,
                    model,
                    rolePrompt,
                },
                {
                    tools,
                    toolChoice: 'auto',
                    maxToolRounds: 3,
                    executeTool: async (toolName, args) => {
                        if (toolName === '$web_search') {
                            return JSON.stringify(args ?? {});
                        }
                        return `æœªçŸ¥å·¥å…·: ${toolName}`;
                    },
                }
            );
        }

        // ç¡®ä¿ rolePrompt ä¸ä¸ºç©º
        const systemPrompt = rolePrompt.trim() ??'ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚';
        const userPrompt = prompt.trim();

        // æ„å»ºæ¶ˆæ¯æ•°ç»„
        const messages = [
            {
                role: "system" as const,
                content: systemPrompt
            },
            {
                role: "user" as const,
                content: userPrompt
            }
        ];

        try {
            const completion = await this.openai.chat.completions.create({
                messages,
                model,
            });
            console.log(completion.choices[0].message);
            return completion.choices[0].message.content ?? '';
        } catch (error) {
            this.logger.error('[AI Service] Error generating AI response:', error);
            return 'è·å–AIå›ç­”å¤±è´¥';
        }
    }

    private safeParseToolArgs(rawArgs: string | undefined): Record<string, unknown> {
        if (!rawArgs) {
            return {};
        }
        try {
            const parsed = JSON.parse(rawArgs);
            if (parsed && typeof parsed === 'object') {
                return parsed as Record<string, unknown>;
            }
        } catch (error) {
            this.logger.warn('[AI Service] Failed to parse tool args', error);
        }
        return {};
    }

    async generateResponseWithTools(
        body: AIRequest,
        tooling: ToolingOptions
    ): Promise<string> {
        const {
            prompt,
            model = process.env.AI_MODEL ?? 'step-3',
            rolePrompt = aiPrompt,
        } = body;

        if (!prompt || prompt.trim() === '') {
            this.logger.error('[AI Service] Empty prompt provided:', { prompt, type: typeof prompt });
            throw new BadRequestException('Prompt cannot be empty');
        }

        const systemPrompt = rolePrompt.trim() ?? 'ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚';
        const userPrompt = prompt.trim();

        const messages: OpenAI.ChatCompletionMessageParam[] = [
            {
                role: 'system',
                content: systemPrompt,
            },
            {
                role: 'user',
                content: userPrompt,
            },
        ];

        const maxRounds = tooling.maxToolRounds ?? 3;
        let lastToolResult = '';
        const startedAt = Date.now();
        let toolRounds = 0;
        let toolCallsCount = 0;
        const toolNames = new Set<string>();
        let usedTools = false;

        const logToolingSummary = (extra?: { errorMessage?: string; endedByMaxRounds?: boolean }) => {
            const durationMs = Date.now() - startedAt;
            const webSearchUsed = toolNames.has('$web_search');
            const payload = {
                model,
                usedTools,
                toolCallsCount,
                toolRounds,
                toolNames: Array.from(toolNames),
                webSearchUsed,
                durationMs,
                ...(extra ?? {}),
            };
            this.logger.log(`[AI Tools] ${JSON.stringify(payload)}`);
        };

        try {
            for (let round = 0; round < maxRounds; round += 1) {
                const requestPayload = {
                    messages,
                    model,
                    tools: tooling.tools,
                    tool_choice: tooling.toolChoice ?? 'auto',
                    stream: false,
                    thinking: {
                        type: 'disabled',
                    },
                    extra_body: {
                        thinking: {
                            type: 'disabled',
                        },
                    },
                } as OpenAI.ChatCompletionCreateParamsNonStreaming & {
                    thinking?: {
                        type?: string;
                    };
                    extra_body?: {
                        thinking?: {
                            type?: string;
                        };
                    };
                };

                const completion = await this.openai.chat.completions.create(requestPayload);

                const message = completion.choices[0]?.message;
                if (!message) {
                    logToolingSummary();
                    return lastToolResult || '';
                }

                if (!message.tool_calls || message.tool_calls.length === 0) {
                    logToolingSummary();
                    return message.content ?? '';
                }

                toolRounds += 1;
                usedTools = true;
                toolCallsCount += message.tool_calls.length;

                const assistantMessage: OpenAI.ChatCompletionMessageParam = {
                    role: 'assistant',
                    content: message.content ?? null,
                    tool_calls: message.tool_calls,
                };
                const reasoningContent = (message as { reasoning_content?: unknown }).reasoning_content;
                (assistantMessage as { reasoning_content?: string }).reasoning_content =
                    typeof reasoningContent === 'string' && reasoningContent.length > 0 ? reasoningContent : ' ';
                messages.push(assistantMessage);

                for (const toolCall of message.tool_calls) {
                    const toolName = toolCall.function?.name;
                    if (!toolName) {
                        continue;
                    }
                    toolNames.add(toolName);
                    const args = this.safeParseToolArgs(toolCall.function.arguments);
                    try {
                        const startedAt = Date.now();
                        this.logger.log('[AI Service] Tool call start', {
                            toolName,
                            args,
                        });
                        const toolResult = await tooling.executeTool(toolName, args);
                        const durationMs = Date.now() - startedAt;
                        this.logger.log('[AI Service] Tool call success', {
                            toolName,
                            durationMs,
                            resultPreview: typeof toolResult === 'string'
                                ? toolResult.slice(0, 200)
                                : '',
                        });
                        lastToolResult = toolResult;
                        const toolMessage: OpenAI.ChatCompletionMessageParam = {
                            role: 'tool',
                            tool_call_id: toolCall.id,
                            content: toolResult,
                        };
                        if (toolName.startsWith('$')) {
                            (toolMessage as { name?: string }).name = toolName;
                        }
                        messages.push(toolMessage);
                    } catch (error) {
                        this.logger.error('[AI Service] Tool execution failed:', error);
                        const errorToolMessage: OpenAI.ChatCompletionMessageParam = {
                            role: 'tool',
                            tool_call_id: toolCall.id,
                            content: 'å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚',
                        };
                        if (toolName.startsWith('$')) {
                            (errorToolMessage as { name?: string }).name = toolName;
                        }
                        messages.push(errorToolMessage);
                    }
                }
            }
        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            logToolingSummary({ errorMessage });
            throw error;
        }

        logToolingSummary({ endedByMaxRounds: true });
        return lastToolResult || 'æœªèƒ½ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼Œè¯·ç¨åé‡è¯•ã€‚';
    }

    private async tavilySearch(args: TavilySearchArgs): Promise<string> {
        if (!args?.query || !args.query.trim()) {
            return 'ç¼ºå°‘ query å‚æ•°';
        }

        const apiKey = process.env.TAVILY_API_KEY;
        if (!apiKey) {
            return 'ç¼ºå°‘ TAVILY_API_KEY é…ç½®';
        }

        const baseUrl = process.env.TAVILY_BASE_URL ?? 'https://api.tavily.com';
        const payload = {
            api_key: apiKey,
            query: args.query.trim(),
            search_depth: args.search_depth ?? 'basic',
            max_results: args.max_results ?? 5,
            include_answer: args.include_answer ?? true,
            include_images: args.include_images ?? false,
            include_raw_content: args.include_raw_content ?? false,
        };

        try {
            const response = await axios.post(`${baseUrl}/search`, payload, {
                timeout: 15000,
            });
            const data = response.data ?? {};
            const results = Array.isArray(data.results) ? data.results : [];
            const normalized = results
                .slice(0, payload.max_results)
                .map((item: any) => ({
                    title: item?.title ?? '',
                    url: item?.url ?? '',
                    content: item?.content ?? item?.snippet ?? '',
                }));

            return JSON.stringify(
                {
                    query: args.query,
                    answer: data.answer ?? '',
                    results: normalized,
                },
                null,
                2
            );
        } catch (error) {
            this.logger.error('[AI Service] Tavily search failed:', error);
            return 'è”ç½‘æœç´¢å¤±è´¥';
        }
    }

    // AI æ€»ç»“èŠå¤©è®°å½•
    async summarizeChatMessages(messages: MessageDto[], selfName?: string, groupName?: string): Promise<string> {
        // è¿‡æ»¤æ¶ˆæ¯å¹¶æ ¼å¼åŒ–
        const formatted = messages
            .filter(m => !selfName || m.sender !== selfName)
            .map(m => `[${m.timestamp}] ${m.sender}: ${m.content}`);

        const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è®°å½•æ€»ç»“åŠ©æ‰‹ã€‚è¯·åˆ†æèŠå¤©è®°å½•å¹¶è¾“å‡ºï¼š

## ğŸ“‹ æ¦‚è§ˆ
ç®€è¦æè¿°æœ¬æ¬¡èŠå¤©çš„ä¸»è¦å†…å®¹ï¼ˆ2-3 å¥è¯ï¼‰

## ğŸ’¬ ä¸»è¦è¯é¢˜
- è¯é¢˜1
- è¯é¢˜2
- è¯é¢˜3

## ğŸ“ å¾…åŠäº‹é¡¹
- [ ] å¾…åŠ1
- [ ] å¾…åŠ2

è¦æ±‚ï¼š
1. è¾“å‡º Markdown æ ¼å¼
2. ç®€æ´ä¸“ä¸š
3. ä¸­æ–‡è¾“å‡º`;

        const chatContent = formatted.join('\n');
        const prompt = groupName
            ? `ç¾¤åï¼š${groupName}\n\nèŠå¤©è®°å½•ï¼š\n${chatContent}`
            : `èŠå¤©è®°å½•ï¼š\n${chatContent}`;

        return await this.generateResponse({
            prompt,
            rolePrompt: systemPrompt,
            model: process.env.AI_MODEL ?? 'step-3'
        });
    }

    // ç”Ÿæˆæ€»ç»“å›¾ç‰‡
    async generateSummaryImage(
        messages: MessageDto[],
        selfName?: string,
        groupName?: string,
        includeRanking: boolean = true
    ): Promise<Buffer> {
        // 1. è¿‡æ»¤æ¶ˆæ¯ + ç»Ÿè®¡å‘è¨€æ•°ï¼ˆå†…è”ï¼‰
        const stats = new Map<string, number>();
        messages
            .filter(m => !selfName || m.sender !== selfName)
            .forEach(m => stats.set(m.sender, (stats.get(m.sender) || 0) + 1));

        // 2. AI æ€»ç»“
        const summary = await this.summarizeChatMessages(messages, selfName, groupName);

        // 3. Markdown è½¬ HTMLï¼ˆç®€åŒ–å†…è”ï¼‰
        const md = new MarkdownIt();
        const contentHTML = summary
            .split(/(?=##\s)/)
            .filter(s => s.trim())
            .map(s => `<div class="section">${md.render(s)}</div>`)
            .join('');

        // 4. ç”Ÿæˆæ’è¡Œæ¦œ HTMLï¼ˆå†…è”ï¼‰
        let rankingHTML = '';
        if (includeRanking) {
            const rankingItems = Array.from(stats.entries())
                .sort((a, b) => b[1] - a[1])
                .slice(0, 10)
                .map((entry, i) => {
                    const rankClass = i === 0 ? 'top-1' : i === 1 ? 'top-2' : i === 2 ? 'top-3' : '';
                    return `
                        <div class="ranking-item">
                            <div class="rank-number ${rankClass}">${i + 1}</div>
                            <div class="rank-info">
                                <span class="rank-name">${entry[0]}</span>
                                <span class="rank-count">${entry[1]} æ¡æ¶ˆæ¯</span>
                            </div>
                        </div>
                    `;
                })
                .join('');

            rankingHTML = `
                <div class="section">
                    <h2>ğŸ† å‘è¨€æ’è¡Œæ¦œ</h2>
                    ${rankingItems}
                </div>
            `;
        }

        // 5. ç”Ÿæˆå®Œæ•´ HTML
        const title = groupName ? `${groupName} èŠå¤©æ€»ç»“` : 'èŠå¤©æ€»ç»“';
        const html = summaryTemplate(title, contentHTML, rankingHTML);

        // 6. è·å– Chrome è·¯å¾„
        const isProduction = process.env.NODE_ENV === 'production' || process.env.VERCEL === '1';
        const executablePath = isProduction
            ? await chromium.executablePath()
            : (() => {
                const localChrome = this.findLocalChrome();
                if (!localChrome) {
                    throw new Error('æ— æ³•æ‰¾åˆ°æœ¬åœ° Chrome å®‰è£…ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ CHROME_PATH æˆ–å®‰è£… Google Chrome');
                }
                return localChrome;
            })();

        // 7. Puppeteer æ¸²æŸ“ï¼ˆä¼˜åŒ–ç­‰å¾…ç­–ç•¥ï¼‰
        const browser = await puppeteer.launch({
            args: isProduction ? chromium.args : ['--no-sandbox', '--disable-setuid-sandbox'],
            defaultViewport: {
                width: 600,
                height: 100,
            },
            executablePath,
            headless: true,
        });

        try {
            const page = await browser.newPage();
            await page.setContent(html, { waitUntil: 'networkidle0' }); // ç­‰å¾…å­—ä½“åŠ è½½å®Œæˆ

            const screenshot = await page.screenshot({
                type: 'jpeg',
                quality: 85,
                fullPage: true,
            });

            return screenshot as Buffer;
        } finally {
            await browser.close();
        }
    }
}
